{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46261657",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    OPTForCausalLM\n",
    ")\n",
    "import gensim\n",
    "from gensim import corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "826e4d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_opt_stories(\n",
    "        model,\n",
    "        tokenizer,\n",
    "        device,\n",
    "        model_name,\n",
    "        real_stories,\n",
    "        num_return_sequence=1,\n",
    "        top_k=16,\n",
    "        story_size_divider=10\n",
    "):\n",
    "    generated_stories = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af69b2fa",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'real_stories' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m each \u001b[38;5;129;01min\u001b[39;00m \u001b[43mreal_stories\u001b[49m:\n\u001b[0;32m      2\u001b[0m     gen_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(each[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'real_stories' is not defined"
     ]
    }
   ],
   "source": [
    "for each in real_stories:\n",
    "    gen_id = 0\n",
    "    print(each['id'])\n",
    "    tokenized_text = tokenizer.encode(each['text'], return_tensors='pt')\n",
    "    story_size = tokenized_text.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd24c5dd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'each' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# First sentence as context\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m first_line \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00meach[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      3\u001b[0m tokenized_first_line \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mencode(\n\u001b[0;32m      4\u001b[0m     first_line,\n\u001b[0;32m      5\u001b[0m     return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      6\u001b[0m     )\n\u001b[0;32m      7\u001b[0m tokenized_first_line \u001b[38;5;241m=\u001b[39m tokenized_first_line\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'each' is not defined"
     ]
    }
   ],
   "source": [
    "# First sentence as context\n",
    "first_line = f\"{each['text'].split('.')[0]}.\"\n",
    "tokenized_first_line = tokenizer.encode(\n",
    "    first_line,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "    \n",
    "tokenized_first_line = tokenized_first_line.to(device)\n",
    "gen = model.generate(\n",
    "    tokenized_first_line,\n",
    "    do_sample=True,\n",
    "    top_k=top_k,\n",
    "    num_return_sequences=num_return_sequence,\n",
    "    max_length=story_size/story_size_divider\n",
    ")\n",
    "\n",
    "outputs = tokenizer.batch_decode(\n",
    "    gen,\n",
    "    skip_special_tokens=True,\n",
    "    clean_up_tokenization_spaces=False\n",
    ")\n",
    "\n",
    "for each_s in outputs:\n",
    "    generated_stories.append({\n",
    "        'id': each['id'],\n",
    "        'gen_id': gen_id,\n",
    "        'p_length': 'first_line',\n",
    "        'model_name': model_name,\n",
    "        'gen_text': each_s,\n",
    "        'prompt': first_line\n",
    "    })\n",
    "    gen_id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90c26847",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (1201468564.py, line 12)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [6]\u001b[1;36m\u001b[0m\n\u001b[1;33m    outputs = tokenizer.batch_decode(\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    " # 10% as context\n",
    " prompt = tokenizer.batch_decode(tokenized_text[:, :story_size//10])\n",
    " tokenized_input = tokenized_text[:, :story_size//10]\n",
    " tokenized_input = tokenized_input.to(device)\n",
    " gen = model.generate(\n",
    "    tokenized_input,\n",
    "    do_sample=True,\n",
    "    top_k=top_k,\n",
    "    num_return_sequences=num_return_sequence,\n",
    "    max_length=story_size/story_size_divider\n",
    "    )\n",
    "    outputs = tokenizer.batch_decode(\n",
    "    gen,\n",
    "    skip_special_tokens=True,\n",
    "    clean_up_tokenization_spaces=False\n",
    "    )\n",
    "    for each_s in outputs:\n",
    "        generated_stories.append({\n",
    "            'id': each['id'],\n",
    "            'gen_id': gen_id,\n",
    "            'p_length': '10_percentage',\n",
    "            'model_name': model_name,\n",
    "            'gen_text': each_s,\n",
    "            'prompt': prompt[0]\n",
    "            })\n",
    "        gen_id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0e016c4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 25% as context\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241m.\u001b[39mbatch_decode(tokenized_text[:, :story_size\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m4\u001b[39m])\n\u001b[0;32m      3\u001b[0m tokenized_input \u001b[38;5;241m=\u001b[39m tokenized_text[:, :story_size\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m4\u001b[39m]\n\u001b[0;32m      4\u001b[0m tokenized_input \u001b[38;5;241m=\u001b[39m tokenized_input\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "# 25% as context\n",
    "prompt = tokenizer.batch_decode(tokenized_text[:, :story_size//4])\n",
    "tokenized_input = tokenized_text[:, :story_size//4]\n",
    "tokenized_input = tokenized_input.to(device)\n",
    "gen = model.generate(\n",
    "    tokenized_input,\n",
    "    do_sample=True,\n",
    "    top_k=top_k,\n",
    "    num_return_sequences=num_return_sequence,\n",
    "    max_length=story_size/story_size_divider\n",
    ")\n",
    "outputs = tokenizer.batch_decode(\n",
    "    gen,\n",
    "    skip_special_tokens=True,\n",
    "    clean_up_tokenization_spaces=False\n",
    ")\n",
    "for each_s in outputs:\n",
    "    generated_stories.append({\n",
    "        'id': each['id'],\n",
    "        'gen_id': gen_id,\n",
    "        'p_length': '25_percentage',\n",
    "        'model_name': model_name,\n",
    "        'gen_text': each_s,\n",
    "        'prompt': prompt[0]\n",
    "    })\n",
    "    gen_id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61bc5442",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 50% as context\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241m.\u001b[39mbatch_decode(tokenized_text[:, :story_size\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m])\n\u001b[0;32m      3\u001b[0m tokenized_input \u001b[38;5;241m=\u001b[39m tokenized_text[:, :story_size\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m      4\u001b[0m tokenized_input \u001b[38;5;241m=\u001b[39m tokenized_input\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "# 50% as context\n",
    "prompt = tokenizer.batch_decode(tokenized_text[:, :story_size//2])\n",
    "tokenized_input = tokenized_text[:, :story_size//2]\n",
    "tokenized_input = tokenized_input.to(device)\n",
    "gen = model.generate(\n",
    "    tokenized_input,\n",
    "    do_sample=True,\n",
    "    top_k=top_k,\n",
    "    num_return_sequences=num_return_sequence,\n",
    "    max_length=story_size/story_size_divider\n",
    ")\n",
    "\n",
    "outputs = tokenizer.batch_decode(\n",
    "    gen,\n",
    "    skip_special_tokens=True,\n",
    "    clean_up_tokenization_spaces=False\n",
    ")\n",
    "\n",
    "for each_s in outputs:\n",
    "    generated_stories.append({\n",
    "        'id': each['id'],\n",
    "        'gen_id': gen_id,\n",
    "        'p_length': '50_percentage',\n",
    "        'model_name': model_name,\n",
    "        'gen_text': each_s,\n",
    "        'prompt': prompt[0]\n",
    "    })\n",
    "    gen_id += 1\n",
    "return generated_stories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "530f5431",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] --model {opt} [--stories_loc STORIES_LOC] [--out_loc OUT_LOC] [--local LOCAL]\n",
      "ipykernel_launcher.py: error: the following arguments are required: --model\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hanna\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\IPython\\core\\interactiveshell.py:3406: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description='Generate stories with context'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--model',\n",
    "        dest='model_name',\n",
    "        choices=[\n",
    "            'opt'\n",
    "        ],\n",
    "        required=True,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--stories_loc',\n",
    "        dest='stories_loc',\n",
    "        default='books/real_processed.json'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--out_loc',\n",
    "        dest='out_loc',\n",
    "        default='books/'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--local',\n",
    "        dest='local',\n",
    "        type=bool,\n",
    "        default=True\n",
    "    )\n",
    "    args = parser.parse_args()\n",
    "    print(args)\n",
    "    if args.local:\n",
    "        if not torch.backends.mps.is_available():\n",
    "            if not torch.backends.mps.is_built():\n",
    "                print(\n",
    "                        \"MPS not available because the current PyTorch install\"\n",
    "                        \" was not built with MPS enabled.\"\n",
    "                    )\n",
    "            else:\n",
    "                print(\n",
    "                        \"MPS not available because the current MacOS version \"\n",
    "                        \"is not 12.3+ and/or you do not have an MPS-enabled \"\n",
    "                        \"device on this machine.\"\n",
    "                    )\n",
    "\n",
    "        else:\n",
    "            device = torch.device(\"mps\")\n",
    "        story_size_divider = 10\n",
    "        num_return_sequences = 1\n",
    "        top_k = 16\n",
    "    else:\n",
    "        device = torch.device(\"cuda:0\")\n",
    "        story_size_divider = 1\n",
    "        num_return_sequences = 10\n",
    "        top_k = 100\n",
    "    real_stories = json.load(open(args.stories_loc))\n",
    "\n",
    "    if args.model_name == 'opt':\n",
    "        if args.local:\n",
    "            model = OPTForCausalLM.from_pretrained(\"facebook/opt-350m\")\n",
    "            tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")\n",
    "        else:\n",
    "            model = OPTForCausalLM.from_pretrained(\"facebook/opt-6.7b\")\n",
    "            tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-6.7b\")\n",
    "        model.to(device)\n",
    "        generated_stories = gen_opt_stories(\n",
    "            model,\n",
    "            tokenizer,\n",
    "            device,\n",
    "            args.model_name,\n",
    "            real_stories,\n",
    "            num_return_sequences,\n",
    "            top_k,\n",
    "            story_size_divider\n",
    "        )\n",
    "        json.dump(\n",
    "            generated_stories,\n",
    "            open(f'{args.out_loc}gen_stories_opt.json', 'w+')\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980d9401",
   "metadata": {},
   "outputs": [],
   "source": [
    "#I need to add this into the above code. TBD\n",
    "\n",
    "dictionary = corpora.Dictionary(generated_stories) # Creating the term dictionary of the corpus, where every unique term is assigned an index. \n",
    "\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean] # Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n",
    "\n",
    "Lda = gensim.models.ldamodel.LdaModel # Creating the object for LDA model using gensim library\n",
    "\n",
    "ldamodel = Lda(doc_term_matrix, num_topics=3, id2word = dictionary, passes=50) # Running and Training LDA model on the document term matrix."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
